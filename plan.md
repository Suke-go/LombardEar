リアルタイム補聴器音声処理の実装計画
プロジェクト概要
このプロジェクトでは、補聴器に近いリアルタイム音声処理システムをPC上でC言語により実装します。マイク入力としてUSBマイクやオーディオインターフェースを用い、リアルタイムで音声を取得します。また、処理した音声はスピーカーから直接出力し、まるで補聴器のように遅延の少ないフィードバックを実現することが目標です。将来的な目標遅延は1ms未満と非常に厳しい値ですが、初期のPC上での試作段階では約10ms程度の遅延を許容します（後述の計算式に基づく） 。ユーザーが音響処理のパラメータを調節できるように、GUI（グラフィカルユーザインタフェース）も実装します。GUIはWeb技術を用い、WebSocket経由でリアルタイムにパラメータを変更できるものとし、HTML/JavaScript + CSSで構成されたシンプルな操作画面を提供する予定です。開発するコードは可能な限りポータブル（移植性が高い）ものとし、WindowsおよびUbuntu Linuxの両プラットフォームで動作するC言語プログラムとして実装します。以下、本計画の各要点について詳述します。
リアルタイム音声入出力設計
図: デジタル補聴器の基本ブロック図の例（マイク入力→信号処理→スピーカー出力）。小型デバイス上の補聴器でもPC上の実装でも、基本的な音声信号の流れは共通しています。マイクロフォンで拾った音声をADC（アナログ-デジタル変換）し、デジタル信号処理（DSP）を施した後、DAC（デジタル-アナログ変換）経由でスピーカーへ出力します。 リアルタイム音声の入出力を実現するため、音声キャプチャと音声再生の処理を連続的に行う必要があります。マイク入力はUSB接続のマイクロフォンやオーディオインターフェースから取得し、プログラム内でバッファに取り込みます。同様に処理後の音声をスピーカーに出力するには、PCの音声出力デバイスへの連続的なストリーミングが必要です。PC上でリアルタイム音声処理を行う場合、一般にダブルバッファリングなどの技術を用いて音声の取りこぼしを防ぎつつ連続再生します
portaudio.com
portaudio.com
。今回の実装でも、入力バッファで音声を読み込みながら別の出力バッファを再生する形で、交互にバッファを切り替えて音途切れを防止する予定です。 音声入出力の方法: 標準Cライブラリのみでは直接オーディオデバイスにアクセスできないため、各OSのネイティブAPIまたは音声ライブラリの使用が必要です。移植性と開発効率を考慮すると、WindowsとLinuxの両方を一つのコードで扱えるクロスプラットフォームの音声ライブラリを採用するのが有力です。例えばオープンソースのPortAudioライブラリは、Windows(WASAPI、WDM-KS、DirectSoundなど)やLinux(ALSA、PulseAudio、JACKなど)各プラットフォームの音声APIを内部でラップし、共通のAPIを提供します
files.portaudio.com
。PortAudioを使えば、コード上はシンプルな関数呼び出しでマイク入力・スピーカー出力のストリームを生成でき、複雑なOS依存処理を避けられます。またPortAudioはC言語で実装され広く利用されており、リアルタイム音声アプリケーションにも実績があります
portaudio.com
。一方で、「可能な限り標準ライブラリのみで実装したい」という要望もあります。PortAudio等の外部ライブラリを導入せずに実現する場合、OSごとの音声APIに直接対応するコードを書き分ける必要があります。具体的には、WindowsではwaveIn/waveOutやWASAPIの使用、LinuxではALSAやPulseAudioの使用、といった実装です。しかしマルチプラットフォーム対応を一から自前で行うのは工数が大きいため、初期段階では無理をせずPortAudioのような軽量ライブラリの力を借りるのが現実的でしょう（PortAudio自体も静的リンク等で組み込めば実行環境への依存は小さく抑えられます）。なお、リアルタイム性を確保するためには排他モードでのデバイス使用（特にWindows WASAPIの場合）や、OSのリアルタイムスケジューリングの活用なども将来的に検討しますが、まずは基本的な入出力ストリームを安定動作させることを優先します。
遅延要件とバッファ設計
本システムの大きな目標の一つは超低遅延です。補聴器の分野では、デジタル処理による音の遅延が大きいと、耳から入る直接音と補聴器からの増幅音がズレて混ざり合いコームフィルタ効果による音質劣化（金属的な響きやこもり感）が生じます
widexpro.com
。従来は「遅延10ms未満であれば許容範囲」とも言われてきましたが
widexpro.com
、近年の研究では5～6ms程度でも音質への悪影響が確認され、可能な限り遅延は短い方が望ましいとされています
widexpro.com
。実際、最新補聴器では0.5ms程度の超低遅延処理を実現し、数ミリ秒の遅延を持つ従来プログラムよりもユーザに好ましい音質を提供しています
widexpro.com
。つまり理想を言えば1ms未満（できれば0.5ms程度）の遅延が目標です
audiologyonline.com
。しかし汎用PC上でソフトウェア実装する場合、OSのスケジューラ遅延や入出力バッファの制約もあり、いきなり1ms未満を達成するのは困難です。そこで初期段階では約10ms以下の遅延を目安とし、実装と動作検証を行います（10msはあくまで一時的な許容値であり、将来的に専用デバイス等で1ms未満を目指します）。 遅延の計算式: システム全体の遅延時間は、主に音声フレームバッファの長さと処理時間によって決まります。おおまかには「遅延(ms) = バッファサイズ(サンプル数) / サンプリングレート × 1000」で求められます。例えばサンプリング周波数48kHzであれば、1msに相当するのは約48サンプルです。したがって1ms未満を実現するには1回の処理あたり数十サンプル以下（例えば32サンプル程度≒0.67ms）のフレーム長で処理を行う必要があります。一方、10msの遅延は48kHzの場合約480サンプルに相当します。一般的なデスクトップOSでは安定動作のため数百～千サンプル単位のバッファを用いることも多いため、10ms(≒480サンプル)は比較的安全側の設定と言えます
portaudio.com
。初期実装では480サンプル程度のフレーム単位で音声を処理し、それによって約10msの遅延となるようにします。その上で動作検証を行い、問題なければ徐々にバッファサイズを縮めて遅延を短縮していきます。実運用では入力側と出力側双方にバッファが存在するため、往復の遅延はこれらの合計になります。例えば入力5ms+出力5ms程度でトータル約10msになるよう各バッファを設定する、といった具合です。システムの調整段階では、実測による遅延評価も行います。オーディオループバックでマイクからスピーカーへの音の遅延を測定し、目標値内に収まっているか確認します。また必要に応じてOSのリアルタイムプロセス設定や割り込みタイマ精度の調整なども検討します。
C言語実装と移植性
ポータブルなコード: PC実装は「CベースのPC実行可能コード（Ubuntu/Windows両対応）」とする方針です。そのため、コード中には特定OSに依存する部分を極力書き込まず、抽象化したインタフェースを設計します。上記の通り音声入出力にはPortAudio等のライブラリを活用する見込みなので、それらの初期化・読取・書込処理をラップした関数を用意しておけば、将来的に別の方法（例えば別ライブラリやハードウェア）に差し替える場合も影響を最小限にできます。標準Cライブラリのみでポータブルに書くこと自体は理想的ですが、音声デバイスやGUIなどどうしても外部依存が必要な箇所があります。その点は無理をせず、信頼性の高い小規模なライブラリ（PortAudioや後述のlibwebsocketsなど）を利用します。 リアルタイム処理: C言語で実装する際はリアルタイム処理におけるスレッドや割り込みへの配慮も必要です。音声の入出力は通常バックグラウンドスレッド（またはコールバック）で連続処理し、メインスレッドはGUIや制御に専念する構成をとります。PortAudioではコールバック関数内で指定したフレーム数ずつ音声が供給されるため、その中でDSP処理を行い即座に出力バッファに書き戻すというループになります。1回のコールバック処理が遅延すると音切れ（バッファアンダーラン）を招くため、DSPアルゴリズムはできるだけ軽量に実装する必要があります。初期段階では複雑な処理（例えば高度なノイズリダクションやビームフォーミングなど）は避け、まずは**シンプルなフィルタ処理や増幅（ボリューム調整）**程度で動作確認します。処理が軽ければバッファ長を短くしても追従でき、低遅延化が図りやすくなります。 DOAの見送り: 質問にあった**DOA(Direction of Arrival, 音源方向推定)**機能については、初期実装では搭載しない予定です。これは主に複数マイクを使った指向性制御の技術ですが、まずは単一マイク入力で基本機能を固めるため、DOA関連の処理（マイクロホンアレイのキャリブレーションやビームフォーミングなど）は行いません。将来的にハードウェアが対応し必要になった段階で追加検討します。現時点ではシステムを単純化し、1チャンネル音声での入出力処理に集中します。
GUI制御（WebSocket経由）
ユーザーがリアルタイムに音響処理のパラメータ（例えば音量やフィルタのカットオフ周波数など）を調整できるよう、GUIを提供します。GUIはWebベースで実装し、WebSocketを用いてバックエンドのCプログラムと通信します。具体的には、Cプログラム内にWebSocketサーバ機能を持たせ、ユーザー側はブラウザで専用のHTMLページを開いて操作する形を想定しています。ブラウザ上のJavaScriptコードがWebSocketを通じて接続し、ユーザー操作に応じたコマンド（JSONメッセージ等）をリアルタイムにC側へ送信します。Cプログラムはそのメッセージを受け取り、例えばボリューム係数を変更したり、フィルタのパラメータを更新したりします。逆にC側から現在の状態をブラウザへ送ることも可能で、これによりGUI上でメーター表示や現在値のフィードバックを行うこともできます。 HTML/JSおよびCSSの用意: WebベースGUIにはHTMLファイルとJavaScriptコード、およびCSSでデザインされたフロントエンドを作成します。初期段階では簡素なページから始め、例えば音量を調節するノブ（つまみ）やスライダーなどを配置します。ユーザーがそれらのUI部品を操作するとJavaScriptからWebSocketでメッセージを送り、C側でパラメータを変更します。HTML/JS側の雛形としては、最低限以下のような要素を含む予定です:
HTML: コントロール部品（例えば<input type="range">やカスタムのノブコンポーネント）とラベル等。ページロード時にWebSocket接続を確立するスクリプトも埋め込みます。
JavaScript: WebSocketオブジェクトを用いてws://でローカルサーバ（Cプログラム）に接続し、onopen, onmessage, oncloseハンドラを定義。ユーザー操作時にsend()でJSON文字列などを送信する。受信時には必要に応じてUIを更新する。
CSS: ノブやスライダーのスタイルを調整し、視覚的に見やすいデザインにします。例えば大きめのフォントや色分けで現在の値がわかりやすいUIにする予定です。必要に応じてシンプルなCSSフレームワークや自前のスタイルシートを用います。
なお、ブラウザ側の実装には特別なフレームワークは使わず、標準的なHTML5+JavaScriptで十分実現可能です。最新のブラウザであればWebSocket APIをサポートしているため、追加のプラグイン等は不要です。 WebSocketサーバの実装方法: Cプログラム内でWebSocketサーバを実装する方法はいくつかあります。ひとつはlibwebsocketsのような既存の軽量ライブラリを使う方法です。Libwebsocketsは純粋なC言語で書かれた軽量なネットワークライブラリで、イベント駆動型で高効率にWebSocketやHTTPサーバを実装できます
libwebsockets.org
。このライブラリを使うと、数百行程度のコードでWebSocketサーバを立ち上げ、特定のパスへの接続を待ち受け、テキストやバイナリメッセージを送受信する処理を書けます。libwebsocketsは静的にも組み込め、ポータブル性も高いため今回の用途に適しています。また、libwebsocketsにはシンプルなHTTPサーバ機能も含まれており、HTML/CSS/JSの静的ファイルをホスティングすることも可能です。そのため、Cプログラム単体でUI用ページを配信しWebSocketも同じポートで扱うオールインワンのサーバとして振る舞えます。 一方、外部ライブラリに頼らず自前で実装する場合は、ソケットプログラミングでHTTPのハンドシェイクを処理しWebSocketプロトコル（データフレーミングやマスキングの処理など）を実装する必要があります。WebSocketは規格(RFC6455)上それほど複雑ではありませんが、ハンドシェイク時のSec-WebSocket-Keyの計算や継続フレームの処理など煩雑な部分もあるため、初期段階で時間をかけるよりは上記ライブラリの利用が得策です。「標準ライブラリのみで」とのポリシーを厳密に適用するなら自前実装になりますが、無理はしない方針とのことですのでlibwebsockets等の活用を提案します。 実装上は、音声処理ループとは別にネットワークサーバ用のスレッドまたはイベントループを走らせます。libwebsocketsの場合はシングルスレッドのイベント駆動でノンブロッキング処理も可能ですが、理解を容易にするためGUI通信は別スレッドで動かし、音声処理スレッドとの間でスレッドセーフなキューや共有変数を介してパラメータの受け渡しをする形も考えられます。これにより、GUI操作によるパラメータ更新が音声処理のリアルタイム性を阻害しないよう配慮します。
ビルドシステムとクロスプラットフォーム対応
異なるOS間でスムーズにビルド・実行できるよう、ビルドシステムの選定も重要です。選択肢としてMakefileとCMakeが挙がっていますが、ここではCMakeを採用することを提案します。CMakeはクロスプラットフォームなビルド構成を記述するのに適しており、一つのCMakeLists.txtからLinux向けのMakefileもWindows向けのVisual Studioソリューション等も生成できます。「安定するほうで」との要望に対し、CMakeは広く使われ成熟しているため安定性は十分です。特にWindows環境ではMakeよりもVisual StudioやMinGW環境が一般的であり、CMakeならそれらに対応できます。CMakeの設定ファイルでは依存ライブラリ（PortAudioやlibwebsocketsなど）の検出やリンク設定も容易に書けるため、将来的に他のプラットフォーム（macOSなど）への拡張も比較的簡単になります。 一方、Makefile単独で書く場合はLinuxでのコンパイルはシンプルですが、Windowsでは別途対応が必要です（MinGWやnmake用Makefileを書くか、プロジェクトファイルを手動で用意する必要があります）。初期段階ではCMakeを用いてプロジェクトを構成し、ソースコードからUbuntu用バイナリとWindows用バイナリの両方をビルドできる体制を整えます。開発時には例えばUbuntu上で開発・テストを行い、適宜Windows環境でもコンパイルして動作確認を行うというワークフローを想定しています。
テスト計画と音声データ
初期の動作確認においては、実験用の音声データセットは特に指定されていません。そのためまずはリアルタイムにマイクから入力しスピーカーへ出力するループが正しく機能するか、人が実際に声を発して確認します。例えばマイクに向かって話した自分の声がほとんど遅れを感じずスピーカーから返ってくることを主観評価します。10ms程度の遅延であれば、人間の感覚ではほぼ違和感のないレベルですが、これが実現できているかを耳で確かめます。また、必要であれば遅延時間を客観的に測定します。オシロスコープや、PC上で録音・再生を同時に行って波形を比べることでマイク入力からスピーカー出力までの遅延をミリ秒精度で測ります。 もし録画済みの音声でテストする場合は、オープンソースの音声データセット（例えば環境音や音声コーパス）や既存のベンチマークデータ（ご提案のあったREVERBデータセットなど※）を再生し、それをマイク入力として拾って処理する形でテストできます。ただし初期段階では必須ではないため、まずはシステム自体の安定動作を優先します。ノイズリダクションやリバーブ抑制といった高度な処理の評価は、システムが完成してから実施する予定です。
※REVERBデータセット: 音声強調（残響低減）の有名な評価用データですが、本プロジェクトではまず基本動作優先のため深追いしません。
まとめ
以上の計画に従い、まずはリアルタイム入出力とGUI制御の骨組みをC言語で構築します。PortAudioによる音声ストリームとlibwebsocketsによる通信機能を統合し、UbuntuおよびWindowsでビルド・実行テストを行います。初期目標の遅延10ms程度を満たしつつ安定して音声が流れることを確認した後、徐々に最適化を行っていきます。具体的には、DSP処理の高速化やバッファ設定の調整により遅延短縮を図り、目標とする1ms未満の超低遅延に近づけます
audiologyonline.com
。また、必要に応じてOS固有の低レイテンシモード（例: WindowsではASIOドライバやWASAPI排他モード、LinuxではJACKの利用など）の検討も行います。GUIについては、基本的なパラメータ調節が可能になった段階で操作性を評価し、必要ならUIレイアウトやデザイン（CSS）の改善を行います。最終的には、補聴器の研究・実験プラットフォームとして使えるような、ポータブルで拡張性の高いリアルタイム音声処理システムを完成させることを目指します。 参考文献・情報源: 本計画書の遅延に関する考察には、補聴器業界の最新知見
widexpro.com
audiologyonline.com
を踏まえています。またPortAudioやWebSocket等技術要素の説明には、それぞれの公式ドキュメント
files.portaudio.com
portaudio.com
libwebsockets.org
を参照しました。